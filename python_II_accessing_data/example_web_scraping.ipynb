{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_web_scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M80WjooLCoSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import all dependencies at the top\n",
        "from time import time\n",
        "from time import sleep\n",
        "from random import randint\n",
        "from IPython.core.display import clear_output\n",
        "from warnings import warn\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "\n",
        "# define a function to process the page\n",
        "def process_page(soup, jobs):  \n",
        "  \n",
        "  # find all elements with class *-job-summary*\n",
        "  raw_jobs = soup.select('.-job-summary')\n",
        "\n",
        "  # same as above, extract the info we need\n",
        "  for job in raw_jobs:\n",
        "    title = job.select_one('.-title > h2 > a').get_text() # extract the title\n",
        "    company = job.select_one('.-company > span:nth-of-type(1)').get_text().strip() # extract the company\n",
        "    tags = [tag.get_text() for tag in job.select('.-tags a')] # extract a list of tags\n",
        "    job = {'title': title, 'company': company, 'tags': tags} # construct a dictionary\n",
        "    jobs.append(job) # add dictionary to list\n",
        "\n",
        "    \n",
        "# prepare for the monitoring logic\n",
        "start_time = time() # note the system time when the program starts\n",
        "request_count = 0 # track the number of requests made\n",
        "\n",
        "# create a list to store the data in\n",
        "jobs = []\n",
        "\n",
        "# variables to handle the request loop\n",
        "has_next_page = True\n",
        "MAX_REQUESTS = 100 # do not request more than 100 pages\n",
        "page_number = 1\n",
        "url = 'https://stackoverflow.com/jobs'\n",
        "headers = {'user-agent': 'jobscraper - school project (myeamail@gmail.com)'}\n",
        "\n",
        "while has_next_page and request_count <= MAX_REQUESTS:\n",
        "  # keep the output clear\n",
        "  clear_output(wait = True)\n",
        "  \n",
        "  # make an initial request\n",
        "  query = {'sort':'i', 'pg': page_number}\n",
        "  response = requests.get(url, params=query, headers=headers)\n",
        "\n",
        "  # make sure we got a valid response\n",
        "  if(response.ok):\n",
        "    # get the full data from the response\n",
        "    data = response.text\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "    process_page(soup, jobs)\n",
        "\n",
        "    # check for the next page\n",
        "    # look for the presence of element with class *test-pagination-next*\n",
        "    next_button = soup.select('.test-pagination-next')\n",
        "    has_next_page = len(next_button) > 0\n",
        "    \n",
        "  else:\n",
        "    # display a warning if there are any problems\n",
        "    warn('Request #: {}, Failed with status code: {}'.format(request_count, response.status_code))\n",
        "  \n",
        "  request_count += 1\n",
        "  \n",
        "  # go to sleep for a bit\n",
        "  # we use a random number between 1 and 5 so\n",
        "  # We can wait as long as 5 seconds to make a second request\n",
        "  \n",
        "  sleep(randint(1,3))\n",
        "  \n",
        "  # output some logs for monitoring\n",
        "  elapsed_time = time() - start_time\n",
        "  print('Requests: {}, Frequency: {} requests/s, {} jobs processed.'.format(request_count, request_count/elapsed_time, len(jobs)))\n",
        "  \n",
        "  # prepare for next iteration\n",
        "  page_number += 1\n",
        "      \n",
        "print('Scraping complete')\n",
        "print('Requests: {}, Frequency: {} requests/s, {} jobs processed.'.format(request_count, request_count/elapsed_time, len(jobs)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}